---
title: "Accepted Papers"
permalink: /2021/accepted-papers/
layout: single
classes: wide
header:
  overlay_color: "#000"
  overlay_filter: "0.1"
  overlay_image: /assets/images/banner.jpg
---

## Workshop on Human Evaluation of NLP Systems

### Short Papers

* [Trading Off Diversity and Quality in Natural Language Generation](https://www.aclweb.org/anthology/2021.humeval-1.3/)
<br />
Hugh Zhang, Daniel Duckworth, Daphne Ippolito and Arvind Neelakantan

* [Towards Objectively Evaluating the Quality of Generated Medical Summaries](https://www.aclweb.org/anthology/2021.humeval-1.6/)
<br />
Francesco Moramarco, Damir Juric, Aleksandar Savkov and Ehud Reiter

* [A Preliminary Study on Evaluating Consultation Notes With Post-Editing](https://www.aclweb.org/anthology/2021.humeval-1.7/)
<br />
Francesco Moramarco, Alex Papadopoulos Korfiatis, Aleksandar Savkov and Ehud Reiter

* [The Great Misalignment Problem in Human Evaluation of NLP Methods](https://www.aclweb.org/anthology/2021.humeval-1.8/)
<br />
Mika Hämäläinen and Khalid Alnajjar

* [Eliciting Explicit Knowledge From Domain Experts in Direct Intrinsic Evaluation of Word Embeddings for Specialized Domains](https://www.aclweb.org/anthology/2021.humeval-1.12/)
<br />
Goya van Boven and Jelke Bloem

* [Detecting Post-Edited References and Their Effect on Human Evaluation](https://www.aclweb.org/anthology/2021.humeval-1.13/)
<br />
Věra Kloudová, Ondřej Bojar and Martin Popel


### Long Papers

* [It's Commonsense, isn't it? Demystifying Human Evaluations in Commonsense-Enhanced NLG systems](https://www.aclweb.org/anthology/2021.humeval-1.1/)
<br />
Miruna-Adriana Clinciu, Dimitra Gkatzia and Saad Mahamood

* [Estimating Subjective Crowd-Evaluations as an Additional Objective to Improve Natural Language Generation](https://www.aclweb.org/anthology/2021.humeval-1.2/)
<br />
Jakob Nyberg, Maike Paetzel and Ramesh Manuvinakurike

* [Towards Document-Level Human MT Evaluation: On the Issues of Annotator Agreement, Effort and Misevaluation](https://www.aclweb.org/anthology/2021.humeval-1.4/)
<br />
Sheila Castilho

* [Is This Translation Error Critical?: Classification-Based Human and Automatic Machine Translation Evaluation Focusing on Critical Errors](https://www.aclweb.org/anthology/2021.humeval-1.5/)
<br />
Katsuhito Sudoh, Kosuke Takahashi and Satoshi Nakamura

* [A View From The Crowd: Evaluation Challenges for Time-Offset Interaction Applications](https://www.aclweb.org/anthology/2021.humeval-1.9/)
<br />
Alberto Chierici and Nizar Habash

* [Reliability of Human Evaluation for Text Summarization: Lessons Learned and Challenges Ahead](https://www.aclweb.org/anthology/2021.humeval-1.10/)
<br />
Neslihan Iskender, Tim Polzehl and Sebastian Möller

* [On User Interfaces for Large-Scale Document-Level Human Evaluation of Machine Translation Outputs](https://www.aclweb.org/anthology/2021.humeval-1.11/)
<br />
Roman Grundkiewicz, Marcin Junczys-Dowmunt, Christian Federmann and Tom Kocmi

* [A Case Study of Efficacy and Challenges in Practical Human-in-Loop Evaluation of NLP Systems Using Checklist](https://www.aclweb.org/anthology/2021.humeval-1.14/)
<br />
Shaily Bhatt, Rahul Jain, Sandipan Dandapat and Sunayana Sitaram

* [Interrater Disagreement Resolution: A Systematic Procedure to Reach Consensus in Annotation Tasks](https://www.aclweb.org/anthology/2021.humeval-1.15/)
<br />
Yvette Oortwijn, Thijs Ossenkoppele and Arianna Betti
